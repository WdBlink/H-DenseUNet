{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liujing/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import random\n",
    "from medpy.io import load\n",
    "import numpy as np\n",
    "import argparse\n",
    "import keras\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from loss import weighted_crossentropy_2ddense\n",
    "import os\n",
    "#from keras.utils2.multi_gpu import make_parallel\n",
    "#from denseunet import DenseUNet\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "#  global parameters\n",
    "parser = argparse.ArgumentParser(description='Keras 2d denseunet Training')\n",
    "#  data folder\n",
    "data='../data/liver'\n",
    "save_path='./Experiments/'\n",
    "#  other paras\n",
    "batch_size =5\n",
    "input_size = 224\n",
    "model_weight = '../model/densenet161_weights_tf.h5'\n",
    "input_cols =3\n",
    "\n",
    "#  data augment\n",
    "MEAN = 48\n",
    "thread_num =14\n",
    "\n",
    "\n",
    "liverlist = [32,34,38,41,47,87,89,91,102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, ZeroPadding2D, concatenate, add\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from lib.custom_layers import Scale\n",
    "\n",
    "def DenseUNet(nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, weights_path=None,\n",
    "              batch_size=None, input_size=None):\n",
    "    '''Instantiate the DenseNet 161 architecture,\n",
    "        # Arguments\n",
    "            nb_dense_block: number of dense blocks to add to end\n",
    "            growth_rate: number of filters to add per dense block\n",
    "            nb_filter: initial number of filters\n",
    "            reduction: reduction factor of transition blocks.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            classes: optional number of classes to classify images\n",
    "            weights_path: path to pre-trained weights\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global concat_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "      concat_axis = 3\n",
    "      img_input = Input(batch_shape=(batch_size, input_size, input_size, 3), name='data')\n",
    "    else:\n",
    "      concat_axis = 1\n",
    "      img_input = Input(shape=(3, 224, 224), name='data')\n",
    "\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    nb_filter = 96\n",
    "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
    "    box = []\n",
    "    # Initial convolution\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    box.append(x)\n",
    "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx+2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        box.append(x)\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
    "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
    "    box.append(x)\n",
    "\n",
    "    up0 = UpSampling2D(size=(2,2))(x)\n",
    "    line0 = Conv2D(2208, (1, 1), padding=\"same\", kernel_initializer=\"normal\", name=\"line0\")(box[3])\n",
    "    up0_sum = add([line0, up0])\n",
    "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up0\")(up0_sum)\n",
    "    bn_up0 = BatchNormalization(name = \"bn_up0\")(conv_up0)\n",
    "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
    "\n",
    "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
    "    up1_sum = add([box[2], up1])\n",
    "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up1\")(up1_sum)\n",
    "    bn_up1 = BatchNormalization(name = \"bn_up1\")(conv_up1)\n",
    "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
    "    up2_sum = add([box[1], up2])\n",
    "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up2\")(up2_sum)\n",
    "    bn_up2 = BatchNormalization(name = \"bn_up2\")(conv_up2)\n",
    "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
    "\n",
    "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
    "    up3_sum = add([box[0], up3])\n",
    "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up3\")(up3_sum)\n",
    "    bn_up3 = BatchNormalization(name = \"bn_up3\")(conv_up3)\n",
    "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
    "\n",
    "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
    "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name=\"conv_up4\")(up4)\n",
    "    conv_up4 = Dropout(rate=0.3)(conv_up4)\n",
    "    bn_up4 = BatchNormalization(name=\"bn_up4\")(conv_up4)\n",
    "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
    "\n",
    "    x = Conv2D(3, (1,1), padding=\"same\", kernel_initializer=\"normal\", name=\"dense167classifer\")(ac_up4)\n",
    "\n",
    "    model = Model(img_input, x, name='denseu161')\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_layers: the number of layers of conv_block to append to the model.\n",
    "            nb_filter: number of filters\n",
    "            growth_rate: growth rate\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    concat_feat = x\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        branch = i+1\n",
    "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
    "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    return concat_feat, nb_filter\n",
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            branch: layer index within each dense block\n",
    "            nb_filter: number of filters\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
    "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_filter: number of filters\n",
    "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "    pool_name_base = 'pool' + str(stage)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base)(x)\n",
    "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count, mini_batch):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        # print (\"data\",data)\n",
    "        # print (\"shape\",shape[:1])\n",
    "        # print (shape[1:])\n",
    "        \n",
    "        # size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        # print (size)\n",
    "        # print ('1',shape[:1] // parts)\n",
    "        # print ('2',shape[1:]*0)\n",
    "        # stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        # print (stride)\n",
    "        # start = stride * idx\n",
    "        # print (start)\n",
    "        # print ('return',tf.slice(data,start,size))\n",
    "        # # exit(0)\n",
    "        # print ('idx', idx*mini_batch,(idx+1)*mini_batch )\n",
    "        return data[idx*mini_batch:(idx+1)*mini_batch,:, :,:]\n",
    "        # data[25:50, :, :, :]\n",
    "        # return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "    # print (outputs_all)\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        id = i\n",
    "        # print ('loading'+str(id))\n",
    "        with tf.device('/gpu:%d' % id):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "                inputs = []\n",
    "                # print ('ssssssssssss')\n",
    "                # print ('rr',model.inputs)\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    # print ('x', x)\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    # print (input_shape)\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    # print ('slice_n', slice_n)\n",
    "                    inputs.append(slice_n)\n",
    "\n",
    "                # print ('ii',inputs)\n",
    "                outputs = model(inputs)\n",
    "                # print ('xx',outputs)\n",
    "\n",
    "                # print ('ssdadsa')\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                # print ('ssd')\n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "                # print ('hard')\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(concatenate(outputs, axis=0))\n",
    "        return Model( outputs=merged, inputs= model.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model......\n",
      "------------------------------\n",
      "Epoch 1/10\n",
      "59/60 [============================>.] - ETA: 1s - loss: 0.4592Epoch 00000: loss improved from inf to 0.45459, saving model to ./Experiments//model/weights.00-0.45.hdf5\n",
      "60/60 [==============================] - 304s - loss: 0.4546   \n",
      "Epoch 2/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.3067Epoch 00001: loss improved from 0.45459 to 0.31778, saving model to ./Experiments//model/weights.01-0.32.hdf5\n",
      "60/60 [==============================] - 45s - loss: 0.3178    \n",
      "Epoch 3/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2585Epoch 00002: loss improved from 0.31778 to 0.25612, saving model to ./Experiments//model/weights.02-0.26.hdf5\n",
      "60/60 [==============================] - 53s - loss: 0.2561    \n",
      "Epoch 4/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2682Epoch 00003: loss did not improve\n",
      "60/60 [==============================] - 48s - loss: 0.2654    \n",
      "Epoch 5/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2016Epoch 00004: loss improved from 0.25612 to 0.20802, saving model to ./Experiments//model/weights.04-0.21.hdf5\n",
      "60/60 [==============================] - 50s - loss: 0.2080    \n",
      "Epoch 6/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2128Epoch 00005: loss did not improve\n",
      "60/60 [==============================] - 46s - loss: 0.2122    \n",
      "Epoch 7/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1788Epoch 00006: loss improved from 0.20802 to 0.19025, saving model to ./Experiments//model/weights.06-0.19.hdf5\n",
      "60/60 [==============================] - 49s - loss: 0.1902    \n",
      "Epoch 8/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2252Epoch 00007: loss did not improve\n",
      "60/60 [==============================] - 47s - loss: 0.2244    \n",
      "Epoch 9/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1859Epoch 00008: loss improved from 0.19025 to 0.18566, saving model to ./Experiments//model/weights.08-0.19.hdf5\n",
      "60/60 [==============================] - 50s - loss: 0.1857    \n",
      "Epoch 10/10\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.1892Epoch 00009: loss did not improve\n",
      "60/60 [==============================] - 46s - loss: 0.1877    \n",
      "Finised Training .......\n"
     ]
    }
   ],
   "source": [
    "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
    "    while 1:\n",
    "        X = np.zeros((batch_size, input_size, input_size, input_cols), dtype='float32')\n",
    "        Y = np.zeros((batch_size, input_size, input_size, 1), dtype='int16')\n",
    "        result_list = []\n",
    "        for idx in range(batch_size):\n",
    "            count = random.choice(trainidx)\n",
    "            img = img_list[count]\n",
    "            tumor = tumor_list[count]\n",
    "            minindex = minindex_list[count]\n",
    "            maxindex = maxindex_list[count]\n",
    "            num = np.random.randint(0,6)\n",
    "            if num < 3 or (count in liverlist):\n",
    "                lines = liverlines[count]\n",
    "                numid = liveridx[count]\n",
    "            else:\n",
    "                lines = tumorlines[count]\n",
    "                numid = tumoridx[count]\n",
    "                \n",
    "            if len(lines)==1:\n",
    "                lines[0] = \"0 0 0\"\n",
    "            #  randomly scale\n",
    "            scale = np.random.uniform(0.8,1.2)\n",
    "            deps = int(input_size * scale)\n",
    "            rows = int(input_size * scale)\n",
    "            cols = 3\n",
    "\n",
    "            sed = np.random.randint(1,numid+1)\n",
    "            cen = lines[sed-1]\n",
    "            cen = np.fromstring(cen, dtype=int, sep=' ')\n",
    "\n",
    "            a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
    "            b = min(max(minindex[1] + rows/2, cen[0]), maxindex[1]- rows/2-1)\n",
    "            c = min(max(minindex[2] + cols/2, cen[0]), maxindex[2]- cols/2-1)\n",
    "\n",
    "            cropp_img = img[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "                        int(c - cols / 2): int(c + cols / 2 + 1)].copy()\n",
    "            cropp_tumor = tumor[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "                          int(c - cols / 2):int(c + cols / 2 + 1)].copy()\n",
    "#             cropp_img = img[:,:,:].copy()\n",
    "#             cropp_tumor = tumor[:,:,:].copy()\n",
    "\n",
    "            cropp_img -= MEAN\n",
    "             # randomly flipping\n",
    "            flip_num = np.random.randint(0, 8)\n",
    "            if flip_num == 1:\n",
    "                cropp_img = np.flipud(cropp_img)\n",
    "                cropp_tumor = np.flipud(cropp_tumor)\n",
    "            elif flip_num == 2:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "            elif flip_num == 3:\n",
    "                cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
    "            elif flip_num == 4:\n",
    "                cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
    "            elif flip_num == 5:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "                cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
    "            elif flip_num == 6:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "                cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
    "            elif flip_num == 7:\n",
    "                cropp_img = np.flipud(cropp_img)\n",
    "                cropp_tumor = np.flipud(cropp_tumor)\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "\n",
    "            cropp_tumor = resize(cropp_tumor, (input_size,input_size,input_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
    "            cropp_img   = resize(cropp_img, (input_size,input_size,input_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
    "            \n",
    "            result_list.append([cropp_img, cropp_tumor[:,:,1]])\n",
    "            \n",
    "        for idx in range(len(result_list)):\n",
    "            X[idx, :, :, :] = result_list[idx][0]\n",
    "            Y[idx, :, :, 0] = result_list[idx][1]\n",
    "        yield (X,Y)\n",
    "\n",
    "\n",
    "def load_fast_files():\n",
    "\n",
    "    trainidx = list(range(41))\n",
    "    img_list = []\n",
    "    tumor_list = []\n",
    "    minindex_list = []\n",
    "    maxindex_list = []\n",
    "    tumorlines = []\n",
    "    tumoridx = []\n",
    "    liveridx = []\n",
    "    liverlines = []\n",
    "    for idx in range(28,69):\n",
    "        img, img_header = load(data+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
    "        tumor, tumor_header = load(data + '/TrainingData/segmentation-' + str(idx) + '.nii')\n",
    "        img_list.append(img)\n",
    "        tumor_list.append(tumor)\n",
    "\n",
    "        maxmin = np.loadtxt(data + '/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
    "        minindex = maxmin[0:3]\n",
    "        maxindex = maxmin[3:6]\n",
    "        minindex = np.array(minindex, dtype='int')\n",
    "        maxindex = np.array(maxindex, dtype='int')\n",
    "        minindex[0] = max(minindex[0] - 3, 0)\n",
    "        minindex[1] = max(minindex[1] - 3, 0)\n",
    "        minindex[2] = max(minindex[2] - 3, 0)\n",
    "        maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
    "        maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
    "        maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
    "        minindex_list.append(minindex)\n",
    "        maxindex_list.append(maxindex)\n",
    "        f1 = open(data + '/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt', 'r')\n",
    "        tumorline = f1.readlines()    #tumor的分割线像素点集合\n",
    "        if len(tumorline)==1:\n",
    "            tumorline = [\"0 0 0\"]\n",
    "        tumorlines.append(tumorline)\n",
    "        tumoridx.append(len(tumorline))\n",
    "        f1.close()\n",
    "        f2 = open(data + '/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt', 'r')\n",
    "        liverline = f2.readlines()   #liver的分割线像素点集合\n",
    "        liverlines.append(liverline)\n",
    "        liveridx.append(len(liverline))\n",
    "        f2.close()\n",
    "\n",
    "    return trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list\n",
    "\n",
    "def train_and_predict():\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "\n",
    "    model = DenseUNet(reduction=0.5, batch_size=batch_size,input_size=input_size)\n",
    "    model.load_weights(model_weight, by_name=True)\n",
    "    #model = make_parallel(model, batch_size//10, mini_batch=10)\n",
    "    sgd = keras.optimizers.SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss=[weighted_crossentropy_2ddense])\n",
    "\n",
    "    trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list = load_fast_files()\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model......')\n",
    "    print('-'*30)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    if not os.path.exists(save_path + \"/model\"):\n",
    "        os.mkdir(save_path + '/model')\n",
    "        os.mkdir(save_path + '/history')\n",
    "    else:\n",
    "        if os.path.exists(save_path+ \"/history/lossbatch.txt\"):\n",
    "            os.remove(save_path + '/history/lossbatch.txt')\n",
    "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
    "            os.remove(save_path + '/history/lossepoch.txt')\n",
    "\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(save_path + '/model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
    "                                       save_best_only=True,save_weights_only=False,mode = 'min', period = 1)\n",
    "\n",
    "    steps = 300/batch_size\n",
    "    model.fit_generator(generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx,\n",
    "                                                  liveridx, minindex_list, maxindex_list),steps_per_epoch=steps,\n",
    "                                                    epochs= 10, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
    "                                                    workers=1, use_multiprocessing=False)\n",
    "\n",
    "    print ('Finised Training .......')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxindex[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minindex[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liverline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(K.set_image_dim_ordering(\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = np.random.uniform(0.8,1.2)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a - deps / 2:a + deps / 2, b - rows / 2:b + rows / 2,\n",
    "        c - cols / 2: c + cols / 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# finial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainidx = list(range(103))\n",
    "img_list = []\n",
    "tumor_list = []\n",
    "minindex_list = []\n",
    "maxindex_list = []\n",
    "tumorlines = []\n",
    "tumoridx = []\n",
    "liveridx = []\n",
    "liverlines = []\n",
    "for idx in range(28,131):\n",
    "    img, img_header = load(data+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
    "    tumor, tumor_header = load(data + '/TrainingData/segmentation-' + str(idx) + '.nii')\n",
    "    img_list.append(img)\n",
    "    tumor_list.append(tumor)\n",
    "\n",
    "    maxmin = np.loadtxt(data + '/myTrainingDataTxt/LiverBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
    "    minindex = maxmin[0:3]\n",
    "    maxindex = maxmin[3:6]\n",
    "    minindex = np.array(minindex, dtype='int')\n",
    "    maxindex = np.array(maxindex, dtype='int')\n",
    "    minindex[0] = max(minindex[0] - 3, 0)\n",
    "    minindex[1] = max(minindex[1] - 3, 0)\n",
    "    minindex[2] = max(minindex[2] - 3, 0)\n",
    "    maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
    "    maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
    "    maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
    "    minindex_list.append(minindex)\n",
    "    maxindex_list.append(maxindex)\n",
    "    f1 = open(data + '/myTrainingDataTxt/TumorPixels/tumor_' + str(idx) + '.txt', 'r')\n",
    "    tumorline = f1.readlines()    #tumor的分割线像素点集合\n",
    "    if len(tumorline)==1:\n",
    "        tumorline = [\"0 0 0\"]\n",
    "    tumorlines.append(tumorline)\n",
    "    tumoridx.append(len(tumorline))\n",
    "    f1.close()\n",
    "    f2 = open(data + '/myTrainingDataTxt/LiverPixels/liver_' + str(idx) + '.txt', 'r')\n",
    "    liverline = f2.readlines()   #liver的分割线像素点集合\n",
    "    liverlines.append(liverline)\n",
    "    liveridx.append(len(liverline))\n",
    "    f2.close()\n",
    "#     print(tumoridx)\n",
    "#     print(liveridx)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(103):\n",
    "    num = np.random.randint(0,6)\n",
    "    if num < 3 or (i in liverlist):\n",
    "            lines = liverlines[i]\n",
    "            numid = liveridx[i]\n",
    "    else:\n",
    "            lines = tumorlines[i]\n",
    "            numid = tumoridx[i]\n",
    "    \n",
    "#     if len(lines)==1:\n",
    "# #         n = 0\n",
    "# #         n = \"%03d\" % n\n",
    "# #         print(n)\n",
    "#         lines[0] = \"0 0 0\"\n",
    "    print(i)\n",
    "    print(\"lines=\",lines[0])\n",
    "    print(\"numid=\",numid)\n",
    "    print(\"len_minindex\",len(minindex))\n",
    "    print(\"len_lines=\",len(lines))\n",
    "    \n",
    "    scale = np.random.uniform(0.8,1.2)\n",
    "    deps = int(input_size * scale)\n",
    "    rows = int(input_size * scale)\n",
    "    cols = 3\n",
    "\n",
    "    sed = np.random.randint(1,numid+1)\n",
    "    print(\"sed=\",sed)\n",
    "    cen = lines[sed-1]\n",
    "    print(\"cen=\",cen)\n",
    "    cen = np.fromstring(cen, dtype=int, sep=' ')\n",
    " \n",
    "    print(\"len_cen\",len(cen))\n",
    "    print(\"len_maxindex\",len(maxindex))\n",
    "    a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
    "    b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
    "    c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
    "    print(\"\")\n",
    "        #test = 208 - 236 / 2:208 + 237 / 2\n",
    "\n",
    "    cropp_img = img[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),int(c - cols / 2):int( c + cols / 2 + 1)].copy()\n",
    "    cropp_tumor = tumor[int(a - deps / 2):int(a + deps / 2),int(b - rows / 2):int(b + rows / 2),int(c - cols / 2):int(c + cols / 2 + 1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(minindex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tumorlines=[]\n",
    "f1 = open(data + '/myTrainingDataTxt/TumorPixels/tumor_' + str(28) + '.txt', 'r')\n",
    "tumorline = f1.readlines()\n",
    "tumorlines.append(tumorline)\n",
    "tumorlines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxmin = np.loadtxt(data + '/myTrainingDataTxt/LiverBox/box_' + str(28) + '.txt', delimiter=' ')\n",
    "minindex = maxmin[0:3]\n",
    "maxindex = maxmin[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "plateau = [2, 3, 1, 4, 1, 4, 2, 3, 4, 1, 3, 2, 3, 2, 4, 1]\n",
    "\n",
    "cy=plateau[1:3].copy()\n",
    "\n",
    "\n",
    "taille = int(sqrt(len(plateau)))\n",
    "\n",
    "print(taille)\n",
    "\n",
    "# Division en lignes\n",
    "L = []\n",
    "i = 1\n",
    "while i < taille:\n",
    "    fin = i * taille\n",
    "    print(\"fine\" ,fin)\n",
    "    debut = fin - taille\n",
    "    print(\"taille %d\",taille)\n",
    "    print(\"debut %d\",debut)\n",
    "    item = plateau[debut:fin]\n",
    "    print(item)\n",
    "    L.append(item)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "int(11/3+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def just_test(list):\n",
    "    happy = list[0]\n",
    "    play = list[1]\n",
    "    return happy,play\n",
    "list = []\n",
    "for i in range(10):\n",
    "    a = i+1\n",
    "    b = 2\n",
    "    list.append([a,b])\n",
    "\n",
    "pool = ThreadPool(thread_num)\n",
    "result_list = pool.map(just_test, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liverlines=[]\n",
    "liveridx=[]\n",
    "tumorlines=[]\n",
    "tumoridx=[]\n",
    "for i in range(28,38):\n",
    "    maxmin = np.loadtxt(data + '/myTrainingDataTxt/LiverBox/box_' + str(i) + '.txt', delimiter=' ')\n",
    "    img, img_header = load(data+ '/myTrainingData/volume-' + str(28) + '.nii')\n",
    "    minindex_list=[]\n",
    "    maxindex_list=[]\n",
    "    minindex = maxmin[0:3]\n",
    "    maxindex = maxmin[3:6]\n",
    "    minindex = np.array(minindex, dtype='int')\n",
    "    maxindex = np.array(maxindex, dtype='int')\n",
    "    minindex[0] = max(minindex[0] - 3, 0)\n",
    "    minindex[1] = max(minindex[1] - 3, 0)\n",
    "    minindex[2] = max(minindex[2] - 3, 0)\n",
    "    maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
    "    maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
    "    maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
    "    minindex_list.append(minindex)\n",
    "    maxindex_list.append(maxindex)\n",
    "    minindex=minindex_list[0]\n",
    "    maxindex=maxindex_list[0]\n",
    "\n",
    "    f1 = open(data + '/myTrainingDataTxt/TumorPixels/tumor_' + str(i) + '.txt', 'r')\n",
    "    tumorline = f1.readlines() \n",
    "    tumorlines.append(tumorline)\n",
    "    tumoridx.append(len(tumorline))\n",
    "    f1.close()\n",
    "    f2 = open(data + '/myTrainingDataTxt/LiverPixels/liver_' + str(i) + '.txt', 'r')\n",
    "    liverline = f2.readlines()\n",
    "    liverlines.append(liverline)\n",
    "    liveridx.append(len(liverline))\n",
    "    f2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(a,b):\n",
    "    for i in range(10):\n",
    "        a = a + 1\n",
    "        b = b + 2\n",
    "    yield a,b\n",
    "c = test(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_arrays_from_file(batch_size, trainidx, img_list, tumor_list, tumorlines, liverlines, tumoridx, liveridx, minindex_list, maxindex_list):\n",
    "    while 1:\n",
    "        X = np.zeros((batch_size, input_size, input_size, input_cols), dtype='float32')\n",
    "        Y = np.zeros((batch_size, input_size, input_size, 1), dtype='int16')\n",
    "        result_list = []\n",
    "        for idx in range(batch_size):\n",
    "            count = random.choice(trainidx)\n",
    "            img = img_list[count]\n",
    "            tumor = tumor_list[count]\n",
    "            minindex = minindex_list[count]\n",
    "            maxindex = maxindex_list[count]\n",
    "            num = np.random.randint(0,6)\n",
    "            if num < 3 or (count in liverlist):\n",
    "                lines = liverlines[count]\n",
    "                numid = liveridx[count]\n",
    "            else:\n",
    "                lines = tumorlines[count]\n",
    "                numid = tumoridx[count]\n",
    "                \n",
    "            #  randomly scale\n",
    "            scale = np.random.uniform(0.8,1.2)\n",
    "            deps = int(input_size * scale)\n",
    "            rows = int(input_size * scale)\n",
    "            cols = 3\n",
    "\n",
    "            sed = np.random.randint(0,numid)\n",
    "            cen = lines[sed-1]\n",
    "            cen = np.fromstring(cen, dtype=int, sep=' ')\n",
    "\n",
    "            a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
    "            b = min(max(minindex[1] + rows/2, cen[1]), maxindex[1]- rows/2-1)\n",
    "            c = min(max(minindex[2] + cols/2, cen[2]), maxindex[2]- cols/2-1)\n",
    "\n",
    "            cropp_img = img[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "                        int(c - cols / 2): int(c + cols / 2 + 1)].copy()\n",
    "            cropp_tumor = tumor[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "                          int(c - cols / 2):int(c + cols / 2 + 1)].copy()\n",
    "\n",
    "            cropp_img -= MEAN\n",
    "             # randomly flipping\n",
    "            flip_num = np.random.randint(0, 8)\n",
    "            if flip_num == 1:\n",
    "                cropp_img = np.flipud(cropp_img)\n",
    "                cropp_tumor = np.flipud(cropp_tumor)\n",
    "            elif flip_num == 2:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "            elif flip_num == 3:\n",
    "                cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
    "            elif flip_num == 4:\n",
    "                cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
    "            elif flip_num == 5:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "                cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=1, axes=(1, 0))\n",
    "            elif flip_num == 6:\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "                cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "                cropp_tumor = np.rot90(cropp_tumor, k=3, axes=(1, 0))\n",
    "            elif flip_num == 7:\n",
    "                cropp_img = np.flipud(cropp_img)\n",
    "                cropp_tumor = np.flipud(cropp_tumor)\n",
    "                cropp_img = np.fliplr(cropp_img)\n",
    "                cropp_tumor = np.fliplr(cropp_tumor)\n",
    "\n",
    "            cropp_tumor = resize(cropp_tumor, (input_size,input_size,input_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
    "            cropp_img   = resize(cropp_img, (input_size,input_size,input_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
    "            \n",
    "            result_list.append([cropp_img, cropp_tumor[:,:,1]])\n",
    "        for idx in range(len(result_list)):\n",
    "            X[idx, :, :, :] = result_list[idx][0]\n",
    "            Y[idx, :, :, 0] = result_list[idx][1]\n",
    "        yield (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lines = [0]\n",
    "lines[0] = \"0 0 0\"\n",
    "#  randomly scale\n",
    "sed = np.random.randint(1,2)\n",
    "cen = lines[sed-1]\n",
    "cen = np.fromstring(cen, dtype=int, sep=' ')\n",
    "cen[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yinpengyu]",
   "language": "python",
   "name": "conda-env-yinpengyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

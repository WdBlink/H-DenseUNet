{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liujing/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import random\n",
    "from medpy.io import load\n",
    "import numpy as np\n",
    "import argparse\n",
    "import keras\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from loss import weighted_crossentropy_2ddense\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import sys\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "#from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "#  global parameters\n",
    "parser = argparse.ArgumentParser(description='Keras 2d denseunet Training')\n",
    "#  data folder\n",
    "data='../data/prostate'\n",
    "save_path='./ProstateExperiments/'\n",
    "image_path='../data/prostate/TrainingData_Part1'\n",
    "#  other paras\n",
    "batch_size = 5\n",
    "input_size = 224\n",
    "model_weight = '../model/densenet161_weights_tf.h5'\n",
    "input_cols =3\n",
    "\n",
    "#  data augment\n",
    "MEAN = 48\n",
    "thread_num =14\n",
    "\n",
    "\n",
    "#liverlist = [32,34,38,41,47,87,89,91,102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, ZeroPadding2D, concatenate, add\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers.pooling import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from lib.custom_layers import Scale\n",
    "\n",
    "def DenseUNet(nb_dense_block=4, growth_rate=48, nb_filter=96, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, weights_path=None,\n",
    "              batch_size=None, input_size=None):\n",
    "    '''Instantiate the DenseNet 161 architecture,\n",
    "        # Arguments\n",
    "            nb_dense_block: number of dense blocks to add to end\n",
    "            growth_rate: number of filters to add per dense block\n",
    "            nb_filter: initial number of filters\n",
    "            reduction: reduction factor of transition blocks.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            classes: optional number of classes to classify images\n",
    "            weights_path: path to pre-trained weights\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global concat_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "      concat_axis = 3\n",
    "      img_input = Input(batch_shape=(batch_size, input_size, input_size, 3), name='data')\n",
    "    else:\n",
    "      concat_axis = 1\n",
    "      img_input = Input(shape=(3, 224, 224), name='data')\n",
    "\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    nb_filter = 96\n",
    "    nb_layers = [6,12,36,24] # For DenseNet-161\n",
    "    box = []\n",
    "    # Initial convolution\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    box.append(x)\n",
    "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx+2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        box.append(x)\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x)\n",
    "    x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x)\n",
    "    box.append(x)\n",
    "\n",
    "    up0 = UpSampling2D(size=(2,2))(x)\n",
    "    line0 = Conv2D(2208, (1, 1), padding=\"same\", kernel_initializer=\"normal\", name=\"line0\")(box[3])\n",
    "    up0_sum = add([line0, up0])\n",
    "    conv_up0 = Conv2D(768, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up0\")(up0_sum)\n",
    "    bn_up0 = BatchNormalization(name = \"bn_up0\")(conv_up0)\n",
    "    ac_up0 = Activation('relu', name='ac_up0')(bn_up0)\n",
    "\n",
    "    up1 = UpSampling2D(size=(2,2))(ac_up0)\n",
    "    up1_sum = add([box[2], up1])\n",
    "    conv_up1 = Conv2D(384, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up1\")(up1_sum)\n",
    "    bn_up1 = BatchNormalization(name = \"bn_up1\")(conv_up1)\n",
    "    ac_up1 = Activation('relu', name='ac_up1')(bn_up1)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2,2))(ac_up1)\n",
    "    up2_sum = add([box[1], up2])\n",
    "    conv_up2 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up2\")(up2_sum)\n",
    "    bn_up2 = BatchNormalization(name = \"bn_up2\")(conv_up2)\n",
    "    ac_up2 = Activation('relu', name='ac_up2')(bn_up2)\n",
    "\n",
    "    up3 = UpSampling2D(size=(2,2))(ac_up2)\n",
    "    up3_sum = add([box[0], up3])\n",
    "    conv_up3 = Conv2D(96, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name = \"conv_up3\")(up3_sum)\n",
    "    bn_up3 = BatchNormalization(name = \"bn_up3\")(conv_up3)\n",
    "    ac_up3 = Activation('relu', name='ac_up3')(bn_up3)\n",
    "\n",
    "    up4 = UpSampling2D(size=(2, 2))(ac_up3)\n",
    "    conv_up4 = Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"normal\", name=\"conv_up4\")(up4)\n",
    "    conv_up4 = Dropout(rate=0.3)(conv_up4)\n",
    "    bn_up4 = BatchNormalization(name=\"bn_up4\")(conv_up4)\n",
    "    ac_up4 = Activation('relu', name='ac_up4')(bn_up4)\n",
    "\n",
    "    x = Conv2D(3, (1,1), padding=\"same\", kernel_initializer=\"normal\", name=\"dense167classifer\")(ac_up4)\n",
    "\n",
    "    model = Model(img_input, x, name='denseu161')\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_layers: the number of layers of conv_block to append to the model.\n",
    "            nb_filter: number of filters\n",
    "            growth_rate: growth rate\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    concat_feat = x\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        branch = i+1\n",
    "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
    "        concat_feat = concatenate([concat_feat, x], axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch))\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    return concat_feat, nb_filter\n",
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            branch: layer index within each dense block\n",
    "            nb_filter: number of filters\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x1')(x)\n",
    "    x = Conv2D(inter_channel, (1, 1), name=conv_name_base+'_x1', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base+'_x2')(x)\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter, (3, 3), name=conv_name_base+'_x2', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_filter: number of filters\n",
    "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "    pool_name_base = 'pool' + str(stage)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base)(x)\n",
    "    x = Conv2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count, mini_batch):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        # print (\"data\",data)\n",
    "        # print (\"shape\",shape[:1])\n",
    "        # print (shape[1:])\n",
    "        \n",
    "        # size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        # print (size)\n",
    "        # print ('1',shape[:1] // parts)\n",
    "        # print ('2',shape[1:]*0)\n",
    "        # stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        # print (stride)\n",
    "        # start = stride * idx\n",
    "        # print (start)\n",
    "        # print ('return',tf.slice(data,start,size))\n",
    "        # # exit(0)\n",
    "        # print ('idx', idx*mini_batch,(idx+1)*mini_batch )\n",
    "        return data[idx*mini_batch:(idx+1)*mini_batch,:, :,:]\n",
    "        # data[25:50, :, :, :]\n",
    "        # return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "    # print (outputs_all)\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        id = i\n",
    "        # print ('loading'+str(id))\n",
    "        with tf.device('/gpu:%d' % id):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "                inputs = []\n",
    "                # print ('ssssssssssss')\n",
    "                # print ('rr',model.inputs)\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    # print ('x', x)\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    # print (input_shape)\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    # print ('slice_n', slice_n)\n",
    "                    inputs.append(slice_n)\n",
    "\n",
    "                # print ('ii',inputs)\n",
    "                outputs = model(inputs)\n",
    "                # print ('xx',outputs)\n",
    "\n",
    "                # print ('ssdadsa')\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                # print ('ssd')\n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "                # print ('hard')\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(concatenate(outputs, axis=0))\n",
    "        return Model( outputs=merged, inputs= model.inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model......\n",
      "------------------------------\n",
      "Epoch 1/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0216Epoch 00000: saving model to ./ProstateExperiments//model/weights.00-0.02.hdf5\n",
      "2000/2000 [==============================] - 1025s - loss: 0.0216  \n",
      "Epoch 2/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0017Epoch 00001: saving model to ./ProstateExperiments//model/weights.01-0.00.hdf5\n",
      "2000/2000 [==============================] - 652s - loss: 0.0017   \n",
      "Epoch 3/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 9.9908e-04Epoch 00002: saving model to ./ProstateExperiments//model/weights.02-0.00.hdf5\n",
      "2000/2000 [==============================] - 650s - loss: 9.9899e-04   \n",
      "Epoch 4/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 6.9930e-04Epoch 00003: saving model to ./ProstateExperiments//model/weights.03-0.00.hdf5\n",
      "2000/2000 [==============================] - 652s - loss: 6.9925e-04   \n",
      "Epoch 5/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 5.3721e-04Epoch 00004: saving model to ./ProstateExperiments//model/weights.04-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 5.3718e-04   \n",
      "Epoch 6/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 4.3588e-04Epoch 00005: saving model to ./ProstateExperiments//model/weights.05-0.00.hdf5\n",
      "2000/2000 [==============================] - 650s - loss: 4.3586e-04   \n",
      "Epoch 7/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 3.6658e-04Epoch 00006: saving model to ./ProstateExperiments//model/weights.06-0.00.hdf5\n",
      "2000/2000 [==============================] - 650s - loss: 3.6657e-04   \n",
      "Epoch 8/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 3.1622e-04Epoch 00007: saving model to ./ProstateExperiments//model/weights.07-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 3.1621e-04   \n",
      "Epoch 9/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 2.7791e-04Epoch 00008: saving model to ./ProstateExperiments//model/weights.08-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 2.7790e-04   \n",
      "Epoch 10/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 2.4781e-04Epoch 00009: saving model to ./ProstateExperiments//model/weights.09-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 2.4780e-04   \n",
      "Epoch 11/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 2.2355e-04Epoch 00010: saving model to ./ProstateExperiments//model/weights.10-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 2.2354e-04   \n",
      "Epoch 12/1000\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 2.0360e-04Epoch 00011: saving model to ./ProstateExperiments//model/weights.11-0.00.hdf5\n",
      "2000/2000 [==============================] - 649s - loss: 2.0359e-04   \n",
      "Epoch 13/1000\n",
      " 833/2000 [===========>..................] - ETA: 378s - loss: 1.9132e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f521b787433a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-f521b787433a>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m                                                    minindex_list, maxindex_list),steps_per_epoch=steps,\n\u001b[1;32m    161\u001b[0m                                                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                                     workers=1, use_multiprocessing=False)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Finised Training .......'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/YinPengyu_file/H-DenseUNet/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/YinPengyu_file/H-DenseUNet/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/YinPengyu_file/H-DenseUNet/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/YinPengyu_file/H-DenseUNet/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yinpengyu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_arrays_from_file(batch_size, trainidx, img_list, prostate_list, prostatelines, prostateidx,  minindex_list, maxindex_list):\n",
    "    while 1:\n",
    "        X = np.zeros((batch_size, input_size, input_size, input_cols), dtype='float32')\n",
    "        Y = np.zeros((batch_size, input_size, input_size, 1), dtype='int16')\n",
    "        result_list = []\n",
    "        for idx in range(batch_size):\n",
    "            count = random.choice(trainidx)\n",
    "            img = img_list[count]\n",
    "            prostate = prostate_list[count]\n",
    "            minindex = minindex_list[count]\n",
    "            maxindex = maxindex_list[count]\n",
    "            \n",
    "            lines = prostatelines[count]\n",
    "            numid = prostateidx[count]\n",
    "                \n",
    "#             if len(lines)==1:\n",
    "#                 lines[0] = \"0 0 0\"\n",
    "            #  randomly scale\n",
    "#             scale = np.random.uniform(0.8,1.2)\n",
    "#             deps = int(input_size * scale)\n",
    "#             rows = int(input_size * scale)\n",
    "#             cols = 3\n",
    "\n",
    "#             sed = np.random.randint(1,numid)\n",
    "#             cen = lines[sed-1]\n",
    "#             cen = np.fromstring(cen, dtype=int, sep=' ')\n",
    "\n",
    "#             a = min(max(minindex[0] + deps/2, cen[0]), maxindex[0]- deps/2-1)\n",
    "#             b = min(max(minindex[1] + rows/2, cen[0]), maxindex[1]- rows/2-1)\n",
    "#             c = min(max(minindex[2] + cols/2, cen[0]), maxindex[2]- cols/2-1)\n",
    "\n",
    "#             cropp_img = img[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "#                         int(c - cols / 2): int(c + cols / 2 + 1)].copy()\n",
    "#             cropp_prostate = prostate[int(a - deps / 2):int(a + deps / 2), int(b - rows / 2):int(b + rows / 2),\n",
    "#                           int(c - cols / 2):int(c + cols / 2 + 1)].copy()\n",
    "\n",
    "#             cropp_img -= MEAN\n",
    "#              # randomly flipping\n",
    "#             flip_num = np.random.randint(0, 8)\n",
    "#             if flip_num == 1:\n",
    "#                 cropp_img = np.flipud(cropp_img)\n",
    "#                 cropp_prostate = np.flipud(cropp_prostate)\n",
    "#             elif flip_num == 2:\n",
    "#                 cropp_img = np.fliplr(cropp_img)\n",
    "#                 cropp_prostate = np.fliplr(cropp_prostate)\n",
    "#             elif flip_num == 3:\n",
    "#                 cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "#                 cropp_prostate = np.rot90(cropp_prostate, k=1, axes=(1, 0))\n",
    "#             elif flip_num == 4:\n",
    "#                 cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "#                 cropp_prostate = np.rot90(cropp_prostate, k=3, axes=(1, 0))\n",
    "#             elif flip_num == 5:\n",
    "#                 cropp_img = np.fliplr(cropp_img)\n",
    "#                 cropp_prostate = np.fliplr(cropp_prostate)\n",
    "#                 cropp_img = np.rot90(cropp_img, k=1, axes=(1, 0))\n",
    "#                 cropp_prostate = np.rot90(cropp_prostate, k=1, axes=(1, 0))\n",
    "#             elif flip_num == 6:\n",
    "#                 cropp_img = np.fliplr(cropp_img)\n",
    "#                 cropp_prostate = np.fliplr(cropp_prostate)\n",
    "#                 cropp_img = np.rot90(cropp_img, k=3, axes=(1, 0))\n",
    "#                 cropp_prostate = np.rot90(cropp_prostate, k=3, axes=(1, 0))\n",
    "#             elif flip_num == 7:\n",
    "#                 cropp_img = np.flipud(cropp_img)\n",
    "#                 cropp_prostate = np.flipud(cropp_prostate)\n",
    "#                 cropp_img = np.fliplr(cropp_img)\n",
    "#                 cropp_prostate = np.fliplr(cropp_prostate)\n",
    "\n",
    "#             cropp_prostate = resize(cropp_prostate, (input_size,input_size,input_cols), order=0, mode='edge', cval=0, clip=True, preserve_range=True)\n",
    "#             cropp_img   = resize(cropp_img, (input_size,input_size,input_cols), order=3, mode='constant', cval=0, clip=True, preserve_range=True)\n",
    "            \n",
    "#             result_list.append([cropp_img, cropp_prostate[:,:,1]])\n",
    "            \n",
    "#         for idx in range(len(result_list)):\n",
    "#             X[idx, :, :, :] = result_list[idx][0]\n",
    "#             Y[idx, :, :, 0] = result_list[idx][1]\n",
    "        yield (X,Y)\n",
    "    \n",
    "\n",
    "\n",
    "def load_fast_files():\n",
    "    trainidx = list(range(50))\n",
    "    img_list = []\n",
    "    prostate_list = []\n",
    "    minindex_list = []\n",
    "    maxindex_list = []\n",
    "    prostatelines = []\n",
    "    prostateidx = []\n",
    "    for idx in range(50):\n",
    "        subject_name = 'Case%02d' % idx\n",
    "        mhd = os.path.join(image_path, subject_name+'.mhd')\n",
    "        img = sitk.ReadImage(mhd)\n",
    "        img = sitk.GetArrayFromImage(img)\n",
    "        #img, img_header = load(data+ '/myTrainingData/volume-' + str(idx) + '.nii')\n",
    "        \n",
    "        subject_name = 'Case%02d' % idx\n",
    "        mhd = os.path.join(image_path, subject_name+'_segmentation.mhd')\n",
    "        prostate = sitk.ReadImage(mhd)\n",
    "        prostate = sitk.GetArrayFromImage(prostate)\n",
    "        \n",
    "        img_list.append(img)\n",
    "        prostate_list.append(prostate)\n",
    "\n",
    "        maxmin = np.loadtxt(data + '/myTrainingDataTxt/ProstateBox/box_' + str(idx) + '.txt', delimiter=' ')\n",
    "        minindex = maxmin[0:3]\n",
    "        maxindex = maxmin[3:6]\n",
    "        minindex = np.array(minindex, dtype='int')\n",
    "        maxindex = np.array(maxindex, dtype='int')\n",
    "        minindex[0] = max(minindex[0] - 3, 0)\n",
    "        minindex[1] = max(minindex[1] - 3, 0)\n",
    "        minindex[2] = max(minindex[2] - 3, 0)\n",
    "        maxindex[0] = min(img.shape[0], maxindex[0] + 3)\n",
    "        maxindex[1] = min(img.shape[1], maxindex[1] + 3)\n",
    "        maxindex[2] = min(img.shape[2], maxindex[2] + 3)\n",
    "        minindex_list.append(minindex)\n",
    "        maxindex_list.append(maxindex)\n",
    "        \n",
    "        f2 = open(data + '/myTrainingDataTxt/ProstatePixels/prostate_' + str(idx) + '.txt', 'r')\n",
    "        prostateline = f2.readlines()   #prostate的分割线像素点集合\n",
    "        prostatelines.append(prostateline)\n",
    "        prostateidx.append(len(prostateline))\n",
    "        f2.close()\n",
    "    return trainidx, img_list, prostate_list, prostatelines, prostateidx, minindex_list, maxindex_list\n",
    "\n",
    "def train_and_predict():\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "\n",
    "    model = DenseUNet(reduction=0.5, batch_size=batch_size,input_size=input_size)\n",
    "    model.load_weights(model_weight, by_name=True)\n",
    "    #model = make_parallel(model, batch_size//10, mini_batch=10)\n",
    "    sgd = keras.optimizers.SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss=[weighted_crossentropy_2ddense])\n",
    "\n",
    "    trainidx, img_list, prostate_list, prostatelines, prostateidx, minindex_list, maxindex_list = load_fast_files()\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model......')\n",
    "    print('-'*30)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    if not os.path.exists(save_path + \"/model\"):\n",
    "        os.mkdir(save_path + '/model')\n",
    "        os.mkdir(save_path + '/history')\n",
    "    else:\n",
    "        if os.path.exists(save_path+ \"/history/lossbatch.txt\"):\n",
    "            os.remove(save_path + '/history/lossbatch.txt')\n",
    "        if os.path.exists(save_path + \"/history/lossepoch.txt\"):\n",
    "            os.remove(save_path + '/history/lossepoch.txt')\n",
    "\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(save_path + '/model/weights.{epoch:02d}-{loss:.2f}.hdf5', monitor='loss', verbose = 1,\n",
    "                                       save_best_only=False,save_weights_only=False,mode = 'min', period = 1)\n",
    "\n",
    "    steps = 10000/batch_size\n",
    "    \n",
    "    model.fit_generator(generate_arrays_from_file(batch_size, trainidx, img_list, prostate_list, prostatelines,  prostateidx,\n",
    "                                                   minindex_list, maxindex_list),steps_per_epoch=steps,\n",
    "                                                    epochs= 50, verbose = 1, callbacks = [model_checkpoint], max_queue_size=10,\n",
    "                                                    workers=1, use_multiprocessing=False)\n",
    "\n",
    "    print ('Finised Training .......')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yinpengyu]",
   "language": "python",
   "name": "conda-env-yinpengyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
